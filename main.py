from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Tuple, Dict, Any
from datetime import datetime, timedelta
import logging
import threading
import re

from scraper import run_scrapers  # <-- tu scraper actual

# ----------------- Logging -----------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Scraper de Alquileres API", version="2.4.0")

# ----------------- CORS -----------------
FRONTEND_ORIGINS = [
    "http://localhost:3000",
    "http://127.0.0.1:3000",
    "https://scraper-alquileres-frontend.vercel.app",
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=FRONTEND_ORIGINS,
    allow_origin_regex=r"https://.*\.vercel\.app",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ----------------- Paginaci√≥n -----------------
DEFAULT_PAGE_SIZE = 20
MAX_PAGE_SIZE = 20  # tope duro

# ----------------- Modelos -----------------
class Property(BaseModel):
    id: str
    titulo: str
    precio: str
    m2: str
    dormitorios: str
    ba√±os: str
    descripcion: str
    link: str
    fuente: str
    scraped_at: str
    imagen_url: str
    is_featured: Optional[bool] = False

class PaginationMeta(BaseModel):
    page: int
    page_size: int
    total: int
    total_pages: int
    has_prev: bool
    has_next: bool

class SearchRequest(BaseModel):
    zona: str
    dormitorios: Optional[str] = "0"
    banos: Optional[str] = "0"
    price_min: Optional[int] = None
    price_max: Optional[int] = None
    palabras_clave: Optional[str] = ""

class SearchResponse(BaseModel):
    success: bool
    count: int
    properties: List[Property]
    meta: PaginationMeta
    message: Optional[str] = None

# ----------------- Helpers -----------------
def clamp_page_size(page_size: Optional[int]) -> int:
    if page_size is None or page_size <= 0:
        return DEFAULT_PAGE_SIZE
    return min(page_size, MAX_PAGE_SIZE)

def clamp_page(page: Optional[int]) -> int:
    return 1 if (page is None or page <= 0) else page

def paginate(items: List[dict], page: int, page_size: int) -> Tuple[List[dict], PaginationMeta]:
    total = len(items)
    total_pages = max(1, (total + page_size - 1) // page_size)
    if page > total_pages:
        page = total_pages
    start = (page - 1) * page_size
    end = start + page_size
    slice_items = items[start:end]
    meta = PaginationMeta(
        page=page,
        page_size=page_size,
        total=total,
        total_pages=total_pages,
        has_prev=page > 1,
        has_next=page < total_pages
    )
    return slice_items, meta

def run_search(
    zona: str,
    dormitorios: str,
    banos: str,
    price_min: Optional[int],
    price_max: Optional[int],
    palabras_clave: str,
) -> List[dict]:
    results = run_scrapers(
        zona=zona,
        dormitorios=dormitorios,
        banos=banos,
        price_min=price_min,
        price_max=price_max,
        palabras_clave=palabras_clave
    )
    if results is None:
        return []
    if hasattr(results, "to_dict"):
        return results.to_dict("records")
    if isinstance(results, list):
        return results
    return []

# ----------------- Heur√≠stica de "destacado" -----------------
FEATURE_KEYWORDS = [
    "piscina", "mascota", "mascotas", "cochera", "estacionamiento",
    "terraza", "balcon", "ascensor", "gimnasio", "amoblado", "amoblada"
]

def score_property(p: Dict[str, Any]) -> float:
    """M√°s puntos si menciona amenities y si el precio (en S/) es menor."""
    score = 0.0
    text = f"{p.get('titulo','')} {p.get('descripcion','')}".lower()
    for kw in FEATURE_KEYWORDS:
        if kw in text:
            score += 1.0

    # precio en soles
    s = str(p.get("precio", ""))
    nums = re.sub(r"[^\d]", "", s)
    if s.strip().upper().startswith("S") and nums:
        try:
            val = int(nums)
            score += max(0.0, 3000.0 / max(val, 1))  # cuanto m√°s bajo, mayor score
        except:
            pass

    # m2 (si lo hay)
    m2txt = str(p.get("m2", ""))
    m2m = re.search(r"(\d{1,4})", m2txt)
    if m2m:
        try:
            m2 = int(m2m.group(1))
            score += min(m2, 120) / 400.0
        except:
            pass
    return score

def mark_featured_one(items: List[dict]) -> List[dict]:
    """Marca exactamente 1 item con is_featured=True dentro de 'items'."""
    if not items:
        return items
    best_idx = None
    best_score = -1e9
    for i, p in enumerate(items):
        sc = score_property(p)
        if sc > best_score:
            best_score = sc
            best_idx = i
    for i, p in enumerate(items):
        p["is_featured"] = (i == best_idx)
    return items

def dedupe_by_link(items: List[dict]) -> List[dict]:
    seen = set()
    out = []
    for p in items:
        link = (p.get("link") or "").strip()
        key = link or (p.get("titulo","") + "|" + p.get("fuente",""))
        if key in seen:
            continue
        seen.add(key)
        out.append(p)
    return out

# ----------------- M√©tricas de b√∫squedas (trending) -----------------
SEARCH_STATS: Dict[str, int] = {}
STATS_LOCK = threading.Lock()

def _stats_key(zona: str, dormitorios: str, banos: str, price_min: Optional[int], price_max: Optional[int], palabras_clave: str) -> str:
    zona = (zona or "").strip().lower()
    dormitorios = str(dormitorios or "0")
    banos = str(banos or "0")
    pmin = "" if price_min is None else str(price_min)
    pmax = "" if price_max is None else str(price_max)
    palabras = (palabras_clave or "").strip().lower()
    return f"{zona}|{dormitorios}|{banos}|{pmin}|{pmax}|{palabras}"

def record_search(zona: str, dormitorios: str, banos: str, price_min: Optional[int], price_max: Optional[int], palabras_clave: str):
    key = _stats_key(zona, dormitorios, banos, price_min, price_max, palabras_clave)
    with STATS_LOCK:
        SEARCH_STATS[key] = SEARCH_STATS.get(key, 0) + 1

def parse_stats_key(key: str) -> Dict[str, Any]:
    zona, dormitorios, banos, pmin, pmax, palabras = key.split("|")
    return {
        "zona": zona,
        "dormitorios": dormitorios if dormitorios else "0",
        "banos": banos if banos else "0",
        "price_min": int(pmin) if pmin else None,
        "price_max": int(pmax) if pmax else None,
        "palabras_clave": palabras or ""
    }

# ----------------- Cache simple con TTL para home-feed -----------------
HOME_CACHE: Dict[str, Any] = {"expires": datetime.min, "payload": None}
HOME_CACHE_TTL = timedelta(minutes=15)

def get_home_cached() -> Optional[dict]:
    if HOME_CACHE["payload"] and datetime.utcnow() < HOME_CACHE["expires"]:
        return HOME_CACHE["payload"]
    return None

def set_home_cached(payload: dict):
    HOME_CACHE["payload"] = payload
    HOME_CACHE["expires"] = datetime.utcnow() + HOME_CACHE_TTL

# ----------------- Rutas b√°sicas -----------------
@app.get("/")
async def root():
    return {"message": "Scraper de Alquileres API", "status": "active"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/sources")
async def list_sources():
    sources = ["nestoria", "infocasas", "urbania", "properati", "doomos"]
    return {"sources": sources}

# ----------------- Endpoints de b√∫squeda -----------------
@app.post("/search", response_model=SearchResponse)
async def search_properties_post(
    request: SearchRequest,
    page: int = Query(1, ge=1, description="P√°gina 1-based"),
    page_size: int = Query(DEFAULT_PAGE_SIZE, ge=1, le=MAX_PAGE_SIZE, description="Tama√±o de p√°gina (m√°x 20)")
):
    try:
        page = clamp_page(page)
        page_size = clamp_page_size(page_size)

        record_search(request.zona, request.dormitorios or "0", request.banos or "0",
                      request.price_min, request.price_max, request.palabras_clave or "")

        properties_all = run_search(
            zona=request.zona,
            dormitorios=request.dormitorios or "0",
            banos=request.banos or "0",
            price_min=request.price_min,
            price_max=request.price_max,
            palabras_clave=request.palabras_clave or ""
        )

        if not properties_all:
            empty_meta = PaginationMeta(page=1, page_size=page_size, total=0, total_pages=1, has_prev=False, has_next=False)
            return SearchResponse(success=True, count=0, properties=[], meta=empty_meta,
                                  message="No se encontraron propiedades que coincidan con los criterios")

        page_items, meta = paginate(properties_all, page, page_size)
        page_items = mark_featured_one(page_items)  # ‚úÖ 1 destacado por p√°gina

        return SearchResponse(
            success=True,
            count=len(page_items),
            properties=page_items,
            meta=meta,
            message=f"Se encontraron {meta.total} propiedades"
        )
    except Exception as e:
        logger.exception("Error en b√∫squeda POST")
        raise HTTPException(status_code=500, detail=f"Error interno del servidor: {str(e)}")

@app.get("/search", response_model=SearchResponse)
async def search_properties_get(
    zona: str = Query(..., description="Zona a buscar"),
    dormitorios: str = Query("0", description="Dormitorios (0 = cualquiera)"),
    banos: str = Query("0", description="Ba√±os (0 = cualquiera)"),
    price_min: Optional[int] = Query(None, description="Precio m√≠nimo (S/)"),
    price_max: Optional[int] = Query(None, description="Precio m√°ximo (S/)"),
    palabras_clave: str = Query("", description="Palabras clave (ej: 'piscina mascotas')"),
    page: int = Query(1, ge=1, description="P√°gina 1-based"),
    page_size: int = Query(DEFAULT_PAGE_SIZE, ge=1, le=MAX_PAGE_SIZE, description="Tama√±o de p√°gina (m√°x 20)")
):
    try:
        page = clamp_page(page)
        page_size = clamp_page_size(page_size)

        record_search(zona, dormitorios, banos, price_min, price_max, palabras_clave)

        properties_all = run_search(
            zona=zona,
            dormitorios=dormitorios,
            banos=banos,
            price_min=price_min,
            price_max=price_max,
            palabras_clave=palabras_clave
        )

        if not properties_all:
            empty_meta = PaginationMeta(page=1, page_size=page_size, total=0, total_pages=1, has_prev=False, has_next=False)
            return SearchResponse(success=True, count=0, properties=[], meta=empty_meta,
                                  message="No se encontraron propiedades que coincidan con los criterios")

        page_items, meta = paginate(properties_all, page, page_size)
        page_items = mark_featured_one(page_items)  # ‚úÖ 1 destacado por p√°gina

        return SearchResponse(
            success=True,
            count=len(page_items),
            properties=page_items,
            meta=meta,
            message=f"Se encontraron {meta.total} propiedades"
        )
    except Exception as e:
        logger.exception("Error en b√∫squeda GET")
        raise HTTPException(status_code=500, detail=f"Error interno del servidor: {str(e)}")

# ----------------- Endpoint: consultas m√°s buscadas -----------------
@app.get("/trending")
async def trending(limit: int = Query(6, ge=1, le=20)):
    """Devuelve combinaciones de b√∫squeda reales, ordenadas por frecuencia."""
    with STATS_LOCK:
        items = sorted(SEARCH_STATS.items(), key=lambda kv: kv[1], reverse=True)[:limit]
    payload = []
    for key, count in items:
        parsed = parse_stats_key(key)
        parsed["count"] = count
        payload.append(parsed)
    return {"items": payload, "generated_at": datetime.utcnow().isoformat()}

# ----------------- Endpoint: home-feed (9 destacados reales) -----------------
@app.get("/home-feed")
async def home_feed():
    """
    Devuelve destacados reales para la portada.
    - Usa top 'trending' si existen; si no, zonas fallback.
    - Ejecuta b√∫squedas reales, crea un pool, lo deduplica y selecciona las 9 con mejor score.
    - Marca cada una con is_featured=True.
    - Tambi√©n devuelve 'sections' por compatibilidad con el front actual.
    """
    cached = get_home_cached()
    if cached:
        return cached

    # 1) Queries base
    with STATS_LOCK:
        sorted_stats = sorted(SEARCH_STATS.items(), key=lambda kv: kv[1], reverse=True)
    base_queries = [parse_stats_key(key) for key, _ in sorted_stats[:3]]

    if not base_queries:
        base_queries = [
            {"zona": "miraflores", "dormitorios": "0", "banos": "0", "price_min": None, "price_max": None, "palabras_clave": ""},
            {"zona": "san isidro", "dormitorios": "0", "banos": "0", "price_min": None, "price_max": None, "palabras_clave": ""},
            {"zona": "santiago de surco", "dormitorios": "0", "banos": "0", "price_min": None, "price_max": None, "palabras_clave": ""},
        ]

    # 2) Ejecutar b√∫squedas y armar pool + secciones
    pool: List[dict] = []
    sections = []
    for q in base_queries:
        try:
            items = run_search(
                zona=q["zona"],
                dormitorios=q["dormitorios"],
                banos=q["banos"],
                price_min=q["price_min"],
                price_max=q["price_max"],
                palabras_clave=q["palabras_clave"]
            )
            # limitar cada secci√≥n a 6 para no sobrecargar
            subset = items[:6]
            sections.append({
                "title": f"{q['zona'].title()}",
                "query": q,
                "count": len(subset),
                "properties": subset
            })
            pool.extend(items[:20])  # tomar hasta 20 por zona para el pool
        except Exception as e:
            logger.warning(f"home-feed secci√≥n fall√≥ para {q}: {e}")

    # 3) Dedup + score -> top 9
    pool = dedupe_by_link(pool)
    scored = [(score_property(p), p) for p in pool]
    scored.sort(key=lambda t: t[0], reverse=True)
    featured = [p for _, p in scored[:9]]
    for p in featured:
        p["is_featured"] = True

    payload = {
        "featured": featured,           # ‚úÖ arreglo de 9 destacados
        "sections": sections,           # compatibilidad
        "generated_at": datetime.utcnow().isoformat(),
        "cached_ttl_minutes": int(HOME_CACHE_TTL.total_seconds() // 60),
    }
    set_home_cached(payload)
    return payload

# ----------------- Ejecuci√≥n local -----------------
if __name__ == "__main__":
    import uvicorn
    print("üöÄ Iniciando servidor FastAPI...")
    print("üìç URL: http://localhost:8000")
    print("üìö Documentaci√≥n: http://localhost:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
